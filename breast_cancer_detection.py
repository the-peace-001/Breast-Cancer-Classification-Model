# -*- coding: utf-8 -*-
"""breast_cancer_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1bS26qqPdolev605n3TGfzUjH_DY1Ui

**Importing Essential Libraries**
"""

#importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Loading Data**"""

#Load breast cancer dataset
from sklearn.datasets import load_breast_cancer
cancer_dataset = load_breast_cancer()

"""**Data Manipulation**"""

cancer_dataset

type(cancer_dataset)

# keys in dataset
cancer_dataset.keys()

# Features of each cell in numeric format
cancer_dataset['data']

type(cancer_dataset['data'])

# malignant or benign value
 # Malignant - cancer Positive / 0
 # Benign - Cancer Negative / 1
cancer_dataset['target']

# target value name malignant or benign tumor
cancer_dataset['target_names']

# Description of Data
print(cancer_dataset['DESCR'])

# Name of features
print(cancer_dataset['feature_names'])

# Location of datafile
print(cancer_dataset['filename'])

"""**Create DataFrame**"""

# Create Dataframe
cancer_df = pd.DataFrame(np.c_[cancer_dataset['data'], cancer_dataset['target']],
                         columns = np.append(cancer_dataset['feature_names'], ['target']))

# DataFrame to CSV file
cancer_df.to_csv('breast_cancer_dataframe.csv')

# Heasd of  cancer DataFrame
cancer_df.head(6)

# Tail of Cancer Dataframe
cancer_df.tail(6)

# Information of Cancer DataFrame
cancer_df.info()

# Numerical Discribution of Data
cancer_df.describe()

cancer_df.isnull().sum()

"""**Data Visualization**"""

# Paiplot of cancer dataframe
sns.pairplot(cancer_df, hue = 'target')

# pairplot of sample feature
sns.pairplot(cancer_df, hue = 'target',
              vars = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness'])

# count the target class
sns.countplot(cancer_df['target'])

# count plot of feature mean radius
plt.figure(figsize = (20, 8))
sns.countplot(cancer_df['mean radius'])

"""**Heat Map**"""

# heatmap of Dataframe
plt.figure(figsize = (16, 9))
sns.heatmap(cancer_df)

cancer_df.corr()

# heatmap of correlation matrix off breast cancer dataframe
plt.figure(figsize = (20,20))
sns.heatmap(cancer_df.corr(), annot = True, cmap = 'coolwarm', linewidths = 2)

"""**Correlation Barplot**"""

# create second dataframe by dopping target
cancer_df2 = cancer_df.drop(['target'], axis = 1)
print("The Shape of cancer_df2 is : ", cancer_df2.shape)

cancer_df2.corrwith(cancer_df.target)

# visualize correlation barplot
plt.figure(figsize = (16,5))
ax = sns.barplot(cancer_df2.corrwith(cancer_df.target))
ax.tick_params(labelrotation = 90)

cancer_df2.corrwith(cancer_df.target).index

"""**Split Dataframe in Train and Test**"""

# input variable
X = cancer_df.drop(['target'], axis = 1)
X.head(6)

# output variable
y = cancer_df['target']
y.head(6)

# split dataset into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)

X_train

X_test

y_train

y_test

"""**Feature Scaling**"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)

"""**Machine Learning Model building**"""

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

"""**Support Vector Classifier**"""

# Support vector classifier
from sklearn.svm import SVC
svc_classifier = SVC()
svc_classifier.fit(X_train, y_train)
y_pred_scv = svc_classifier.predict(X_test)
accuracy_score(y_test, y_pred_scv)

# Train with standard scaled Data
svc_classifier2 = SVC()
svc_classifier2.fit(X_train_sc, y_train)
y_pred_svc_sc = svc_classifier2.predict(X_test_sc)
accuracy_score(y_test, y_pred_svc_sc)

"""**Logistic Regression**"""

# Logistic Regression
from sklearn.linear_model import LogisticRegression
lr_classifier = LogisticRegression(random_state = 51, penalty = 'l2')
lr_classifier.fit(X_train, y_train)
y_pred_lr = lr_classifier.predict(X_test)
accuracy_score(y_test, y_pred_lr)

# Train with Standard scaled Data
lr_classifier2 = LogisticRegression(random_state = 51 , penalty = 'l2')
lr_classifier2.fit(X_train_sc, y_train)
y_pred_lr_sc = lr_classifier.predict(X_test_sc)
accuracy_score(y_test, y_pred_lr_sc)

"""**K-Nearest Neighbour classifier**"""

# K-Nearest Neighbor classifier
from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
knn_classifier.fit(X_train, y_train)
y_pred_knn = knn_classifier.predict(X_test)
accuracy_score(y_test, y_pred_knn)

# Train with Standard Classifier
knn_classifier2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
knn_classifier2.fit(X_train_sc, y_train)
y_pred_knn_sc = knn_classifier.predict(X_test_sc)
accuracy_score(y_test, y_pred_knn_sc)

"""**Naive Bayes Classifier**"""

# Naive Bayes Classier
from sklearn.naive_bayes import GaussianNB
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)
y_pred_nb = nb_classifier.predict(X_test)
accuracy_score(y_test, y_pred_nb)

# Train with Standard scaled data
nb_classifier2 = GaussianNB()
nb_classifier2.fit(X_train_sc, y_train)
y_pred_nb_sc = nb_classifier.predict(X_test)
accuracy_score(y_test, y_pred_nb_sc)

"""**Decision Tree Classifier**

"""

# Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 51)
dt_classifier.fit(X_train, y_train)
y_pred_dt = dt_classifier.predict(X_test)
accuracy_score(y_test, y_pred_dt)

# Train with standard scale data
dt_classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 51)
dt_classifier2.fit(X_train_sc, y_train)
y_pred_dt_sc = dt_classifier.predict(X_test_sc)
accuracy_score(y_test, y_pred_dt_sc)

"""**Random Forest Classifier**

"""

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 50)
rf_classifier.fit(X_train, y_train)
y_pred_rf = rf_classifier.predict(X_test)
accuracy_score(y_test, y_pred_rf)

# Train with standard scale data
rf_classifier2 = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 50)
rf_classifier2.fit(X_train_sc, y_train)
y_pred_rf_sc = rf_classifier.predict(X_test_sc)
accuracy_score(y_test, y_pred_rf_sc)

"""**AdaBoost Classifier**"""

# AdaBoost Classifier
from sklearn.ensemble import AdaBoostClassifier
adb_classifier = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', random_state = 21), n_estimators = 2000, learning_rate = 0.1, algorithm = "SAMME.R", random_state = 1)
adb_classifier.fit(X_train, y_train)
y_pred_adb = adb_classifier.predict(X_test)
accuracy_score(y_test, y_pred_adb)

# Train with standard scaled data
adb_classifier2 = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', random_state = 21), n_estimators = 2000, learning_rate = 0.1, algorithm = "SAMME.R", random_state = 1)
adb_classifier2.fit(X_train_sc, y_train)
y_pred_adb_sc = adb_classifier.predict(X_test_sc)
accuracy_score(y_test, y_pred_adb_sc)

"""**XGBoost Classifier**"""

# XGBoost Classifier
from xgboost import XGBClassifier
xgb_classifier = XGBClassifier()
xgb_classifier.fit(X_train, y_train)
y_pred_xgb = xgb_classifier.predict(X_test)
accuracy_score(y_test, y_pred_xgb)

# Train with standard scaled data
xgb_classifier2 = XGBClassifier()
xgb_classifier2.fit(X_train_sc, y_train)
y_pred_xgb_sc = xgb_classifier.predict(X_test_sc)
accuracy_score(y_test, y_pred_xgb_sc)

"""**XGBoost Parameter Tuning Randomized Search**"""

params = {
    "learning_rate" : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],
    "max_depth" : [3,4,5,6,8,10,12,15],
    "min_child_weight" : [1,3, 5,7],
    "gamma" : [0.0, 0.1, 0.2, 0.3, 0.4],
    "colsample_bytree" : [0.3, 0.4, 0.5, 0.7]
}

# Randomized search
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(xgb_classifier, param_distributions= params, scoring= 'roc_auc', n_jobs=-1, verbose = 3)
random_search.fit(X_train, y_train)

random_search.best_params_

random_search.best_estimator_

# Training xgboost classifier with best parameters
xgb_classifier_pt = XGBClassifier(base_score=0.5, booster = "gbtree", colsample_bylevel = 1,
                                  colsample_bynode = 1, colsample_bytree = 0.4, gamma = 0.2,
                                  learning_rate =0.1 , max_delta_step = 0, max_depth = 15,
                                  min_child_weight = 1, missing = np.nan, n_estimators = 100, n_jobs = 1,
                                  nthread = None, objective = "binary:logistic", random_state = 0,
                                  reg_alpha = 0, reg_lambda = 1, scale_pos_weight = 1, seed = None,
                                  silent = None, subsample = 1, verbosity = 1)
xgb_classifier_pt.fit(X_train, y_train)
y_pred_xgb_pt = xgb_classifier_pt.predict(X_test)
accuracy_score(y_test, y_pred_xgb_pt)

"""**Grid Search**"""

from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(xgb_classifier, param_grid = params, scoring = 'roc_auc', n_jobs = -1, verbose  = 3)
grid_search.fit(X_train, y_train)

grid_search.best_estimator_

xgb_classifier_pt_gs = XGBClassifier(base_score=0.5, booster = "gbtree", colsample_bylevel = 1,
                                  colsample_bynode = 1, colsample_bytree = 0.3, gamma = 0.0,
                                  learning_rate =0.3 , max_delta_step = 0, max_depth = 3,
                                  min_child_weight = 1, missing = np.nan, n_estimators = 100, n_jobs = 1,
                                  nthread = None, objective = "binary:logistic", random_state = 0,
                                  reg_alpha = 0, reg_lambda = 1, scale_pos_weight = 1, seed = None,
                                  silent = None, subsample = 1, verbosity = 1)
xgb_classifier_pt_gs.fit(X_train, y_train)
y_pred_xgb_pt_gs = xgb_classifier_pt_gs.predict(X_test)
accuracy_score(y_test, y_pred_xgb_pt_gs)

cm = confusion_matrix(y_test, y_pred_xgb_pt)
plt.title('Heatmap of Confusion Matrix', fontsize = 15)
sns.heatmap(cm, annot = True)
plt.show()

"""**Classification Report of Model**"""

print(classification_report(y_test, y_pred_xgb_pt))

"""**Cross Validation of ML Model**"""

# Cross validation
from sklearn.model_selection import cross_val_score
cross_validation = cross_val_score(estimator = xgb_classifier_pt_gs, X = X_train, y = y_train, cv = 10)
print("Cross Validation Accuracy : {:.3f} %".format(cross_validation.mean()*100))
print("Cross Validation Standard Deviation : {:.3f} %".format(cross_validation.std()*100))

## pickle
import pickle

# save model
pickle.dump(xgb_classifier_pt, open('breast_cancer_detector.pickle', 'wb'))

# load model
breast_cancer_detector_model = pickle.load(open('breast_cancer_detector.pickle', 'rb'))

# predict the output
y_pred = breast_cancer_detector_model.predict(X_test)

# confusion matrix
print('Confusion matrix of XGBoost model: \n',confusion_matrix(y_test, y_pred),'\n')

# show the accuracy
print('Accuracy of XGBoost model = ',accuracy_score(y_test, y_pred))





